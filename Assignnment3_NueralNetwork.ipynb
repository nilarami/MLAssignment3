{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OtUVK0LIU1XC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ucimlrepo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm1-DiJEzSdt",
        "outputId": "3d244262-baa5-4be1-d78d-b8ead28152d7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.6-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import learning_curve, validation_curve\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n"
      ],
      "metadata": {
        "id": "6QTB19RCzTE0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grab_breast_cancer_dataset():\n",
        "  # fetch dataset\n",
        "  breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)\n",
        "  # data (as pandas dataframes)\n",
        "  X = breast_cancer_wisconsin_diagnostic.data.features\n",
        "  y = breast_cancer_wisconsin_diagnostic.data.targets\n",
        "\n",
        "  return X,y\n",
        "\n",
        "def preprocess_data(X, y, test_size=0.2, random_state=None):\n",
        "\n",
        "  #Numerize y data\n",
        "  y['Diagnosis'] = y['Diagnosis'].map({'M': 1, 'B': 0})\n",
        "\n",
        "  # Split the data into training and testing sets\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "  y_train = y_train.values.reshape(-1)\n",
        "  y_test = y_test.values.reshape(-1)\n",
        "\n",
        "\n",
        "  return x_train, x_test, y_train, y_test\n",
        "\n",
        "def evaluate_classifier(classifier, x_train, y_train, x_test, y_test):\n",
        "    # Make predictions on the training data\n",
        "    y_pred = classifier.predict(x_train)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(y_train, y_pred)\n",
        "    precision = precision_score(y_train, y_pred)\n",
        "    recall = recall_score(y_train, y_pred)\n",
        "    f1 = f1_score(y_train, y_pred)\n",
        "\n",
        "    #Print the metrics: Training\n",
        "    print(\"Training Results\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "\n",
        "    # Make predictions on the testing data\n",
        "    y_pred = classifier.predict(x_test)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    # Print the metrics\n",
        "    print(\"Validation Test Results\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    return accuracy, precision, recall, f1\n"
      ],
      "metadata": {
        "id": "uJoe7gYa5Lri"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Functions for Nueral Network\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "def grid_search_neural_network(X_train, y_train, cv=5):\n",
        "\n",
        "    param_grid = {\n",
        "        'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  # Number of neurons in each hidden layer\n",
        "        'activation': ['relu', 'tanh'],                                 # Activation function\n",
        "        'solver': ['adam', 'sgd'],                                      # Solver for weight optimization\n",
        "        'learning_rate': ['constant', 'adaptive'],                      # Learning rate schedule\n",
        "        'alpha': [0.0001, 0.001, 0.01],                                 # L2 penalty (regularization term)\n",
        "    }\n",
        "\n",
        "    param_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'solver': ['adam'],\n",
        "    'learning_rate': ['constant'],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    }\n",
        "\n",
        "    nn_classifier = MLPClassifier(max_iter=1000)\n",
        "\n",
        "    grid_search = GridSearchCV(nn_classifier, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_params = grid_search.best_params_\n",
        "    best_estimator = grid_search.best_estimator_\n",
        "\n",
        "    return best_params, best_estimator\n"
      ],
      "metadata": {
        "id": "lUgqGu6T5KgT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the best nueral network: best_nn_classifier\n",
        "#hyperparameters: Best Hyperparameters for Nueral Network: {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
        "\n",
        "#Baseline\n",
        "import time\n",
        "\n",
        "\n",
        "X,y = grab_breast_cancer_dataset()\n",
        "x_train, x_test, y_train, y_test = preprocess_data(X, y, test_size=0.2, random_state=42)\n",
        "# Measure the start time\n",
        "start_time = time.time()\n",
        "\n",
        "nn_classifier = MLPClassifier(\n",
        "    activation='relu',\n",
        "    alpha=0.01,\n",
        "    hidden_layer_sizes=(50,),\n",
        "    learning_rate='constant',\n",
        "    solver='adam'\n",
        ")\n",
        "\n",
        "nn_classifier.fit(x_train, y_train)\n",
        "\n",
        "# Measure the end time\n",
        "end_time = time.time()\n",
        "\n",
        "evaluate_classifier(nn_classifier, x_train, y_train, x_test, y_test)\n",
        "\n",
        "# Calculate the execution time\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "print(f\"Execution time: {execution_time:.2f} seconds\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pjdVfGOHtS5",
        "outputId": "1fa08c76-2f9c-4027-c70e-ef8a5af2671d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Results\n",
            "Accuracy: 0.9319\n",
            "F1 Score: 0.9063\n",
            "Validation Test Results\n",
            "Accuracy: 0.9649\n",
            "F1 Score: 0.9524\n",
            "Execution time: 1.25 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply PCA to Reduce Dataset then run Nueral Network Again\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"PCA\")\n",
        "\n",
        "X,y = grab_breast_cancer_dataset()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA(n_components=10)\n",
        "X = pca.fit_transform(X)\n",
        "\n",
        "x_train, x_test, y_train, y_test = preprocess_data(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Measure the start time\n",
        "start_time = time.time()\n",
        "\n",
        "nn_classifier = MLPClassifier(\n",
        "    activation='relu',\n",
        "    alpha=0.01,\n",
        "    hidden_layer_sizes=(50,),\n",
        "    learning_rate='constant',\n",
        "    solver='adam'\n",
        ")\n",
        "\n",
        "nn_classifier.fit(x_train, y_train)\n",
        "\n",
        "# Measure the end time\n",
        "end_time = time.time()\n",
        "\n",
        "evaluate_classifier(nn_classifier, x_train, y_train, x_test, y_test)\n",
        "\n",
        "# Calculate the execution time\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "print(f\"Execution time: {execution_time:.2f} seconds\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyGqJsCQJvEs",
        "outputId": "d5812010-6a17-44af-9369-44dbf117f895"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA\n",
            "Training Results\n",
            "Accuracy: 0.9912\n",
            "F1 Score: 0.9880\n",
            "Validation Test Results\n",
            "Accuracy: 0.9737\n",
            "F1 Score: 0.9655\n",
            "Execution time: 0.93 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply ICA to Reduce Dataset then run Nueral Network Again\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import FastICA\n",
        "\n",
        "print(\"ICA\")\n",
        "\n",
        "\n",
        "X,y = grab_breast_cancer_dataset()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "ica = FastICA(n_components=30, random_state=42)  # Specify the number of components to keep\n",
        "X = ica.fit_transform(X)\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = preprocess_data(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Measure the start time\n",
        "start_time = time.time()\n",
        "\n",
        "nn_classifier = MLPClassifier(\n",
        "    activation='relu',\n",
        "    alpha=0.01,\n",
        "    hidden_layer_sizes=(50,),\n",
        "    learning_rate='constant',\n",
        "    solver='adam'\n",
        ")\n",
        "\n",
        "nn_classifier.fit(x_train, y_train)\n",
        "\n",
        "# Measure the end time\n",
        "end_time = time.time()\n",
        "\n",
        "evaluate_classifier(nn_classifier, x_train, y_train, x_test, y_test)\n",
        "\n",
        "# Calculate the execution time\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "print(f\"Execution time: {execution_time:.2f} seconds\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs32hXbQKgk7",
        "outputId": "e0ba5aa5-a936-448a-b988-7ba17d174d07"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ICA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_fastica.py:542: FutureWarning: Starting in v1.3, whiten='unit-variance' will be used by default.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Results\n",
            "Accuracy: 0.9714\n",
            "F1 Score: 0.9605\n",
            "Validation Test Results\n",
            "Accuracy: 0.9561\n",
            "F1 Score: 0.9398\n",
            "Execution time: 1.22 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply Random Projection to Reduce Dataset then run Nueral Network Again\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "\n",
        "\n",
        "print(\"Random Projection\")\n",
        "\n",
        "\n",
        "X,y = grab_breast_cancer_dataset()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "rp = GaussianRandomProjection(n_components=30)\n",
        "X = rp.fit_transform(X)\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = preprocess_data(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Measure the start time\n",
        "start_time = time.time()\n",
        "\n",
        "nn_classifier = MLPClassifier(\n",
        "    activation='relu',\n",
        "    alpha=0.01,\n",
        "    hidden_layer_sizes=(50,),\n",
        "    learning_rate='constant',\n",
        "    solver='adam'\n",
        ")\n",
        "\n",
        "nn_classifier.fit(x_train, y_train)\n",
        "\n",
        "# Measure the end time\n",
        "end_time = time.time()\n",
        "\n",
        "evaluate_classifier(nn_classifier, x_train, y_train, x_test, y_test)\n",
        "\n",
        "# Calculate the execution time\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "print(f\"Execution time: {execution_time:.2f} seconds\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK6r3PlkL9hM",
        "outputId": "944ac152-e5e0-46c6-edaa-13a69a0561a5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Projection\n",
            "Training Results\n",
            "Accuracy: 0.9736\n",
            "F1 Score: 0.9641\n",
            "Validation Test Results\n",
            "Accuracy: 0.9474\n",
            "F1 Score: 0.9302\n",
            "Execution time: 1.44 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply Isomap to Reduce Dataset then run Nueral Network Again\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "from sklearn.manifold import Isomap\n",
        "\n",
        "\n",
        "print(\"Isomap\")\n",
        "\n",
        "\n",
        "X,y = grab_breast_cancer_dataset()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "isomap = Isomap(n_components=2)\n",
        "X = isomap.fit_transform(X)\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = preprocess_data(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Measure the start time\n",
        "start_time = time.time()\n",
        "\n",
        "nn_classifier = MLPClassifier(\n",
        "    activation='relu',\n",
        "    alpha=0.01,\n",
        "    hidden_layer_sizes=(50,),\n",
        "    learning_rate='constant',\n",
        "    solver='adam'\n",
        ")\n",
        "\n",
        "nn_classifier.fit(x_train, y_train)\n",
        "\n",
        "# Measure the end time\n",
        "end_time = time.time()\n",
        "\n",
        "evaluate_classifier(nn_classifier, x_train, y_train, x_test, y_test)\n",
        "\n",
        "# Calculate the execution time\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "print(f\"Execution time: {execution_time:.2f} seconds\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyLQPPfTMcXW",
        "outputId": "bc708f7c-f2c0-4ec5-f429-fb4b616c693f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Isomap\n",
            "Training Results\n",
            "Accuracy: 0.9670\n",
            "F1 Score: 0.9552\n",
            "Validation Test Results\n",
            "Accuracy: 0.9649\n",
            "F1 Score: 0.9512\n",
            "Execution time: 0.37 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5"
      ],
      "metadata": {
        "id": "iIRQyFv8YJG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Fetch the dataset\n",
        "X,y = grab_breast_cancer_dataset()\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply Isomap to reduce the dataset\n",
        "isomap = Isomap(n_components=2)\n",
        "X_reduced = isomap.fit_transform(X_scaled)\n",
        "\n",
        "# Cluster the reduced dataset with KMeans\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(X_reduced)\n",
        "\n",
        "# Concatenate cluster labels with the reduced dataset\n",
        "X_with_clusters = np.column_stack((X_reduced, cluster_labels))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_with_clusters, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the neural network classifier\n",
        "start_time = time.time()\n",
        "\n",
        "nn_classifier = MLPClassifier(\n",
        "    activation='relu',\n",
        "    alpha=0.01,\n",
        "    hidden_layer_sizes=(50,),\n",
        "    learning_rate='constant',\n",
        "    solver='adam'\n",
        ")\n",
        "\n",
        "nn_classifier.fit(x_train, y_train)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Evaluate the classifier\n",
        "train_accuracy = nn_classifier.score(x_train, y_train)\n",
        "test_accuracy = nn_classifier.score(x_test, y_test)\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Execution Time: {execution_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aue2cBkTYKT7",
        "outputId": "83f5b4fd-2c1d-4b9e-ad3b-4a9d0a292ecc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9670\n",
            "Test Accuracy: 0.9649\n",
            "Execution Time: 1.22 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "\n",
        "# Fetch the dataset\n",
        "X,y = grab_breast_cancer_dataset()\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply Isomap to reduce the dataset\n",
        "isomap = Isomap(n_components=2)\n",
        "X_reduced = isomap.fit_transform(X_scaled)\n",
        "\n",
        "gmm = GaussianMixture(n_components=2, random_state=10)\n",
        "cluster_labels = gmm.fit_predict(X_reduced)\n",
        "\n",
        "# Concatenate cluster labels with the reduced dataset\n",
        "X_with_clusters = np.column_stack((X_reduced, cluster_labels))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_with_clusters, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the neural network classifier\n",
        "start_time = time.time()\n",
        "\n",
        "nn_classifier = MLPClassifier(\n",
        "    activation='relu',\n",
        "    alpha=0.01,\n",
        "    hidden_layer_sizes=(50,),\n",
        "    learning_rate='constant',\n",
        "    solver='adam'\n",
        ")\n",
        "\n",
        "nn_classifier.fit(x_train, y_train)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Evaluate the classifier\n",
        "train_accuracy = nn_classifier.score(x_train, y_train)\n",
        "test_accuracy = nn_classifier.score(x_test, y_test)\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Execution Time: {execution_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVROsmHcaB_W",
        "outputId": "dd12ea9a-67c1-40b5-decf-0752ed917ae8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9714\n",
            "Test Accuracy: 0.9649\n",
            "Execution Time: 0.88 seconds\n"
          ]
        }
      ]
    }
  ]
}